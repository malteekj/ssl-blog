---
layout: page
title:  "Contrastive Learning Models"
date:   #2022-02-01 21:52:57 +0100
permalink: /contrastive-learning-models/
---

### **Introduction**

**Content**
* [Intuition](#intuition)  
* [Model and Math](#model-and-math)  
* [Seminal Papers](#seminal-papers)  
* [Model Variations](#variations)  
* [Tricks of the Trade](#tricks-of-the-trade)  
* [Code Repositories](#code-repositories)  
&nbsp;

### **Intuition**<a name="#intuition"></a>
<!--- Your text here --->
&nbsp; 

### **Model and Math**<a name="#model-and-math"></a>
**Important Models:**
- SimCLR
- BYOL
- CLIP

**Key concepts:**
- Heavy augmentations 
- Hard negative mining 
- Large batch sizes
- Approaches in Vision
    - Parallel Augmentation (SimCLR, BYOL, Barlow Twins)
    - Memory Bank (MoCo-V2, CURL)
    - Feature Clustering (DeepCluster, SwAV)
    - "Supervised" methods (CLIP)
- Approaches in Language:
    - Text augmentation
    - Back-translation
    - Dropout and cut-off (SimCSE)
    - Supervision from NLI (alot of BERT)
    - Unsupervised Sentence Embedding Learning (context prediction, mutual information)


&nbsp;

### **Seminal Papers**<a name="#seminal-papers"></a>
&nbsp;

### **Model Variations**<a name="#variations"></a>
&nbsp;

### **Tricks of the Trade**<a name="#tricks-of-the-trade"></a>
&nbsp;


### **Code Repositories**<a name="#code-repositories"></a>



